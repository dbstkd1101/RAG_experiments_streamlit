import os
import streamlit as st
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain.document_loaders import PyPDFLoader
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
import tempfile

os.environ["OPENAI_API_KEY"] = "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"


#PDF íŒŒì¼ ë¡œë“œ ë° ë¶„í• 
@st.cache_resource
def load_and_split_pdf(pdf_path):
    loader = PyPDFLoader(pdf_path)
    return loader.load_and_split()

@st.cache_resource
def create_vector_store(_docs):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = text_splitter.split_documents(_docs)
    persist_directory = "./chroma_db"
    vectorstore = Chroma.from_documents(
        split_docs,
        OpenAIEmbeddings(model = 'text-embedding-3-small'),
        persist_directory=persist_directory
    )
    return vectorstore

#ë§Œì•½ ê¸°ì¡´ì— ì €ì¥í•´ë‘” CrhomaDBê°€ ìˆëŠ” ê²½ìš°, ì´ë¥¼ ë¡œë“œ
@st.cache_resource
def get_vector_store(_docs):
    persist_directory = "./chroma_db"
    if os.path.exists(persist_directory):
        return Chroma(persist_directory=persist_directory, embedding_function=OpenAIEmbeddings(model = 'text-embedding-3-small'))
    else:
        return create_vector_store(_docs)

#Document ê°ì²´ì˜ page_contentë¥¼ Join
def format_docs(docs):
    return "\n".join([doc.page_content for doc in docs])

@st.cache_resource
def load_pdf(_file):
    #ì„ì‹œ íŒŒì¼ì„ ìƒì„±í•˜ì—¬ ì—…ë¡œë“œëœ PDF íŒŒì¼ì˜ ë°ì´í„°ë¥¼ ì €ì¥
    with tempfile.NamedTemporaryFile(mode="wb", delete=False) as tmp_file:
        #ì—…ë¡œë“œëœ íŒŒì¼ì˜ ë‚´ìš©ì„ ì„ì‹œ íŒŒì¼ì— ê¸°ë¡
        tmp_file.write(_file.getvalue())
        #ì„ì‹œ íŒŒì¼ì˜ ê²½ë¡œë¥¼ ë³€ìˆ˜ì— ì €ì¥
        tmp_file_path = tmp_file.name
        #ì„ì‹œ íŒŒì¼ì˜ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  í˜ì´ì§€ë¥¼ ë¶„í• 
        loader = PyPDFLoader(file_path=tmp_file_path)
        pages = loader.load_and_split()
        
    return pages

#Initialize the LangChain components
def chaining(_pages):
    # file_path = r"./ëŒ€í•œë¯¼êµ­í—Œë²•(í—Œë²•)(ì œ00010í˜¸)(19880225).pdf"
    
    # pages = load_and_split_pdf(file_path)
    vector_store = get_vector_store(pages)
    retriever = vector_store.as_retriever(search_kwargs={"k": 3})

    #Define the answer question prompt
    qa_system_prompt = """
    You are an asisttant for question-answering tasks.\
    Use the following pieces of retrieved context to answer the question.\
    If you don't know the answer, just say that you don't know.\
    Keep the answer perfect. please use imogi with the answer.
    Please answer in Korean and use respectful language.\
    {context}
    """

    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            ("human", "{input}"),
        ]
    )

    llm = ChatOpenAI(model="gpt-4o", temperature=0)
    rag_chain = (
        {"context": retriever | format_docs, "input": RunnablePassthrough()}
        | qa_prompt
        | llm
        | StrOutputParser()
    )
    return rag_chain

# Streamlit UI êµ¬ì„±
st.title("ğŸ’¬ Chatbot")
st.header("í—Œë²• Q&A ì±—ë´‡ ğŸ”‰")
uploaded_file = st.file_uploader("Upload a PDF", type=["pdf"])

if uploaded_file is not None:
    pages = load_pdf(uploaded_file)
    
    rag_chain = chaining(pages)

    #session_stateì— messages Keyê°’ ì§€ì • ë° Streamlit í™”ë©´ ì§„ì… ì‹œ, AIì˜ ì¸ì‚¬ë§ì„ ê¸°ë¡í•˜ê¸°
    if "messages" not in st.session_state:
        st.session_state["messages"] = [{"role": "assistant", "content": "í—Œë²•ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"}]

    #ì‚¬ìš©ìë‚˜ AIê°€ ì§ˆë¬¸/ë‹µë³€ì„ ì£¼ê³ ë°›ì„ ì‹œ, ì´ë¥¼ ê¸°ë¡í•˜ëŠ” session_state
    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])

    if prompt_message := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš” :)"):
        st.chat_message("human").write(prompt_message)
        st.session_state.messages.append({"role": "user", "content": prompt_message})
        with st.chat_message("ai"):
            with st.spinner("Thinking..."):
                response = rag_chain.invoke(prompt_message)
                st.session_state.messages.append({"role": "assistant", "content": response})
                st.write(response)    


# #chat_input()ì— ì…ë ¥ê°’ì´ ìˆëŠ” ê²½ìš°,
# if prompt := st.chat_input():
#     #messagesë¼ëŠ” session_stateì— ì—­í• ì€ ì‚¬ìš©ì, ì»¨í…ì¸ ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ê°ê° ì €ì¥
#     st.session_state.messages.append({"role": "user", "content": prompt})
#     #chat_message()í•¨ìˆ˜ë¡œ ì‚¬ìš©ì ì±„íŒ… ë²„ë¸”ì— prompt ë©”ì‹œì§€ë¥¼ ê¸°ë¡
#     st.chat_message("user").write(prompt)

    
#     response = chat.invoke(prompt)
#     msg = response.content

#     #messagesë¼ëŠ” session_stateì— ì—­í• ì€ AI, ì»¨í…ì¸ ëŠ” APIë‹µë³€ì„ ê°ê° ì €ì¥
#     st.session_state.messages.append({"role": "assistant", "content": msg})
#     #chat_message()í•¨ìˆ˜ë¡œ AI ì±„íŒ… ë²„ë¸”ì— API ë‹µë³€ì„ ê¸°ë¡
#     st.chat_message("assistant").write(msg)


            