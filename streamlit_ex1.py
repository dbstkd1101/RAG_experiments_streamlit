import os
import streamlit as st
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

os.environ["OPENAI_API_KEY"] = "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"


#PDF íŒŒì¼ ë¡œë“œ ë° ë¶„í• 
@st.cache_resource
def load_and_split_pdf(pdf_path):
    loader = PyPDFLoader(pdf_path)
    return loader.load_and_split()

@st.cache_resource
def create_vector_store(docs):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = text_splitter.split_documents(docs)
    persist_directory = "./chroma_db"
    vectorstore = Chroma.from_documents(
        split_docs,
        OpenAIEmbeddings(model = 'text-embedding-3-small'),
        persist_directory=persist_directory
    )
    return vectorstore

#ë§Œì•½ ê¸°ì¡´ì— ì €ì¥í•´ë‘” CrhomaDBê°€ ì‡ëŠ” ê²½ìš°, ì´ë¥¼ ë¡œë“œ
def get_vector_store(_docs):
    persist_directory = "./chroma_db"
    if os.path.exists(persist_directory):
        return Chroma(persist_directory=persist_directory, embedding_function=OpenAIEmbeddings(model = 'text-embedding-3-small'))
    else:
        return create_vector_store(_docs)

#Document ê°ì²´ì˜ page_contentë¥¼ Join
def format_docs(docs):
    return "\n".join([doc.page_content for doc in docs])

def chaining():
    file_path = r"/Users/LeeYunSang/vscode/ëŒ€í•œë¯¼êµ­í—Œë²•(í—Œë²•)(ì œ00010í˜¸)(19880225).pdf"
    pages = load_and_split_pdf(file_path)
    vector_store = get_vector_store(pages)
    retriever = vector_store.as_retriever(search_kwargs={"k": 3})

    qa_system_prompt = """
    You are an asisttant for question-answering tasks.\
    Use the following pieces of retrieved context to answer the question.\
    If you don't know the answer, just say that you don't know.\
    Keep the answer perfect. please use imogi with the answer.
    Please answer in Korean and use respectful language.\
    {context}
    """

    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            ("human", "{input}"),
        ]
    )

    llm = ChatOpenAI(model="gpt-4o", temperature=0)
    rag_chain = (
        {"context": retriever | format_docs, "input": RunnablePassthrough()}
        | qa_prompt
        | llm
        | StrOutputParser()
    )

    return rag_chain

# Streamlit UI
st.header("í—Œë²• Q&A ì±—ë´‡")
rag_chain = chaining()

st.title("ğŸ’¬ Chatbot")

#session_stateì— messages Keyê°’ ì§€ì • ë° Streamlit í™”ë©´ ì§„ì… ì‹œ, AIì˜ ì¸ì‚¬ë§ì„ ê¸°ë¡í•˜ê¸°
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "í—Œë²•ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"}]

#ì‚¬ìš©ìë‚˜ AIê°€ ì§ˆë¬¸/ë‹µë³€ì„ ì£¼ê³ ë°›ì„ ì‹œ, ì´ë¥¼ ê¸°ë¡í•˜ëŠ” session_state
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])


#chat_input()ì— ì…ë ¥ê°’ì´ ìˆëŠ” ê²½ìš°,
if prompt := st.chat_input():
    #messagesë¼ëŠ” session_stateì— ì—­í• ì€ ì‚¬ìš©ì, ì»¨í…ì¸ ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ê°ê° ì €ì¥
    st.session_state.messages.append({"role": "user", "content": prompt})
    #chat_message()í•¨ìˆ˜ë¡œ ì‚¬ìš©ì ì±„íŒ… ë²„ë¸”ì— prompt ë©”ì‹œì§€ë¥¼ ê¸°ë¡
    st.chat_message("user").write(prompt)

    
    response = chat.invoke(prompt)
    msg = response.content

    #messagesë¼ëŠ” session_stateì— ì—­í• ì€ AI, ì»¨í…ì¸ ëŠ” APIë‹µë³€ì„ ê°ê° ì €ì¥
    st.session_state.messages.append({"role": "assistant", "content": msg})
    #chat_message()í•¨ìˆ˜ë¡œ AI ì±„íŒ… ë²„ë¸”ì— API ë‹µë³€ì„ ê¸°ë¡
    st.chat_message("assistant").write(msg)